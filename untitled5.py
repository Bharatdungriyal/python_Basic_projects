# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XtwDJYCz4Uj6SzStcjltStX26Lr99HIo
"""

#1lab Experiment
# To implement AND function using Perceptron
import numpy as np

# AND function
inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
outputs = np.array([0, 0, 0, 1])

# Initialize weights and bias
weights = np.array([0.0, 0.0])
bias = 0.0
learning_rate = 0.1

# Training
for epoch in range(100):
    for x, y in zip(inputs, outputs):
        # Calculate the output
        linear_output = np.dot(x, weights) + bias
        y_pred = 1 if linear_output >= 0 else 0

        # Update weights and bias
        error = y - y_pred
        weights += learning_rate * error * x
        bias += learning_rate * error

print(f"Weights: {weights}")
print(f"Bias: {bias}")

# Testing
for x in inputs:
    linear_output = np.dot(x, weights) + bias
    y_pred = 1 if linear_output >= 0 else 0
    print(f"Input: {x}, Predicted Output: {y_pred}")

#2 lab NOR Gate implementation with binary input and bipolar target using adaline.
import numpy as np

# NOR gate
inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
targets = np.array([-1, 1, 1, 1])  # Bipolar targets

# Convert binary inputs to bipolar (-1 and 1)
inputs = 2 * inputs - 1

# Initialize weights and bias
weights = np.random.rand(2)
bias = np.random.rand(1)
learning_rate = 0.1

# Training
for epoch in range(100):
    for x, target in zip(inputs, targets):
        # Calculate the output
        linear_output = np.dot(x, weights) + bias
        error = target - linear_output

        # Update weights and bias
        weights += learning_rate * error * x
        bias += learning_rate * error

print(f"Weights: {weights}")
print(f"Bias: {bias}")

# Testing
for x in inputs:
    linear_output = np.dot(x, weights) + bias
    output = np.sign(linear_output)
    print(f"Input: {x}, Predicted Output: {output}")

# 3 lab XOR Gate implementation with bipolar input and bipolar target using madaline.
import numpy as np
import pandas as pd

# Activation function
def activation_fn(z):
    return 1 if z >= 0 else -1

# Madaline function
def Madaline(Input, Target, lr, epoch):
    weight = np.random.random((Input.shape[1], Input.shape[1]))
    bias = np.random.random(Input.shape[1])
    w = np.array([0.5 for i in range(weight.shape[1])])
    b = 0.5
    k = 0

    while k < epoch:
        error = []
        z_input = np.zeros(bias.shape[0])
        z = np.zeros(bias.shape[0])

        for i in range(Input.shape[0]):
            for j in range(Input.shape[1]):
                z_input[j] = sum(weight[j] * Input[i]) + bias[j]
                z[j] = activation_fn(z_input[j])

            y_input = sum(z * w) + b
            y = activation_fn(y_input)

            # Update the weight & bias
            if y != Target[i]:
                for j in range(weight.shape[1]):
                    weight[j] = weight[j] + lr * (Target[i] - z_input[j]) * Input[i]
                    bias[j] = bias[j] + lr * (Target[i] - z_input[j])

            # Store squared error value
            error.append((Target[i] - y_input) ** 2)

        # Compute sum of squared error
        Error = sum(error)
        print(k, '>> Error :', Error)
        k += 1

    return weight, bias

# Input dataset
x = np.array([[1.0, 1.0, 1.0], [1.0, -1.0, 1.0], [-1.0, 1.0, 1.0], [-1.0, -1.0, -1.0]])
# Target values
t = np.array([1, 1, 1, -1])

# Train the Madaline
w, b = Madaline(x, t, 0.0001, 3)
print('Weights:', w)
print('Bias:', b)

# 4 lab Experiment
"""Create a perceptron with appropriate no. of inputs and outputs. Train it using
fixed increment learning algorithm until no change in weights is required. Output the final weights."""
import numpy as np

# Step activation function
def activation_fn(z):
    return 1 if z >= 0 else 0

# Perceptron training using fixed increment learning algorithm
def train_perceptron(inputs, outputs, learning_rate, epochs):
    weights = np.random.rand(inputs.shape[1])
    bias = np.random.rand(1)

    for epoch in range(epochs):
        errors = 0
        for x, target in zip(inputs, outputs):
            linear_output = np.dot(x, weights) + bias
            y_pred = activation_fn(linear_output)
            error = target - y_pred
            if error != 0:
                weights += learning_rate * error * x
                bias += learning_rate * error
                errors += 1
        if errors == 0:
            break

    return weights, bias

# Input dataset (AND function)
inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
outputs = np.array([0, 0, 0, 1])

# Train the perceptron
learning_rate = 0.1
epochs = 100
final_weights, final_bias = train_perceptron(inputs, outputs, learning_rate, epochs)

print('Final Weights:', final_weights)
print('Final Bias:', final_bias)

# Testing
for x in inputs:
    linear_output = np.dot(x, final_weights) + final_bias
    y_pred = activation_fn(linear_output)
    print(f'Input: {x}, Predicted Output: {y_pred}')

#5 lab Experiment
""" Using back-propagation network, find the new weights.
It is presented with the input pattern [0, 1] and the target output is
1. Use a learning rate α = 0.25 and binary sigmoidal activation function.
"""

# Sigmoid activation function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Derivative of sigmoid function
def sigmoid_derivative(x):
    return x * (1 - x)

# Parameters
alpha = 0.25  # Learning rate
input_pattern = np.array([0, 1])  # Input pattern
target_output = 1  # Target output
initial_weights = np.array([0.5, 0.3, 0.8, 0.6])  # Initial weights

# Initial weights for simplicity
w1, w2, w3, w4 = initial_weights[0], initial_weights[1], initial_weights[2], initial_weights[3]

# Step 1: Feedforward pass
net_h1 = (input_pattern[0] * w1) + (input_pattern[1] * w2)
net_h2 = (input_pattern[0] * w3) + (input_pattern[1] * w4)
output_h1 = sigmoid(net_h1)
output_h2 = sigmoid(net_h2)

# For simplicity, let's assume w5 and w6 are initialized as 0.5
w5, w6 = 0.5, 0.5
net_o1 = (output_h1 * w5) + (output_h2 * w6)
output_o1 = sigmoid(net_o1)

# Step 2: Calculate error
error = target_output - output_o1

# Step 3: Backpropagate error and update weights
delta_o1 = output_o1 * (1 - output_o1) * error
new_w5 = w5 + (alpha * delta_o1 * output_h1)
new_w6 = w6 + (alpha * delta_o1 * output_h2)

# Calculate delta for hidden layer
delta_h1 = output_h1 * (1 - output_h1) * (delta_o1 * w5)
delta_h2 = output_h2 * (1 - output_h2) * (delta_o1 * w6)

# Update input to hidden layer weights
new_w1 = w1  # input_pattern[0] is 0, so no update
new_w2 = w2 + (alpha * delta_h1 * input_pattern[1])
new_w3 = w3  # input_pattern[0] is 0, so no update
new_w4 = w4 + (alpha * delta_h2 * input_pattern[1])

# Output results
print(f"Initial Output: {output_o1}")
print(f"Error: {error}")
print(f"Updated Weights: w1={new_w1}, w2={new_w2}, w3={new_w3}, w4={new_w4}, w5={new_w5}, w6={new_w6}")

# 6 Program for to perform Union,Intersection and Complement operations in Fuzzy Sets
import numpy as np

# Define fuzzy sets A and B
A = np.array([0.1, 0.4, 0.6, 0.9])
B = np.array([0.7, 0.5, 0.2, 0.3])

# Union of A and B
def fuzzy_union(A, B):
    return np.maximum(A, B)

# Intersection of A and B
def fuzzy_intersection(A, B):
    return np.minimum(A, B)

# Complement of A
def fuzzy_complement(A):
    return 1 - A

# Perform the operations
union_result = fuzzy_union(A, B)
intersection_result = fuzzy_intersection(A, B)
complement_result_A = fuzzy_complement(A)
complement_result_B = fuzzy_complement(B)

# Print the results
print(f"Fuzzy Set A: {A}")
print(f"Fuzzy Set B: {B}")
print(f"Union (A ∪ B): {union_result}")
print(f"Intersection (A ∩ B): {intersection_result}")
print(f"Complement of A: {complement_result_A}")
print(f"Complement of B: {complement_result_B}")

#7 Write programe that ask user to enter two fuzzy sets and computes the resultant fuzzy relation upto 10 greater than 6.
import numpy as np

def get_fuzzy_set(name):
    fuzzy_set = {}
    for i in range(1, 11):
        membership_value = float(input(f"Enter the membership value for {i} in {name}: "))
        fuzzy_set[i] = membership_value
    return fuzzy_set

def fuzzy_relation(set_a, set_b):
    relation = {}
    for x in set_a:
        if x > 6:
            relation[x] = min(set_a[x], set_b[x])
    return relation

def main():
    print("Enter the membership values for Fuzzy Set A (values between 0 and 1):")
    fuzzy_set_a = get_fuzzy_set("Set A")

    print("\nEnter the membership values for Fuzzy Set B (values between 0 and 1):")
    fuzzy_set_b = get_fuzzy_set("Set B")

    result_relation = fuzzy_relation(fuzzy_set_a, fuzzy_set_b)

    print("\nThe resultant fuzzy relation for elements greater than 6 is:")
    for x, val in result_relation.items():
        print(f"({x}, {val})")

if __name__ == "__main__":
    main()
6365
4

#8 lab Experiment
"""Create two matrices of the dimension 3x3 and 3x4 respectively which contain random number as there elements.
Compute composition of these two fuzzy relation using both max-min and max-product composition."""
import numpy as np

def generate_random_matrix(rows, cols):
    """Generate a matrix with random values between 0 and 1."""
    return np.random.rand(rows, cols)

def max_min_composition(A, B):
    """Compute max-min composition of matrices A and B."""
    rows_A, cols_A = A.shape
    rows_B, cols_B = B.shape
    assert cols_A == rows_B, "Number of columns in A must be equal to number of rows in B"

    C = np.zeros((rows_A, cols_B))
    for i in range(rows_A):
        for j in range(cols_B):
            C[i, j] = np.max(np.minimum(A[i, :], B[:, j]))

    return C

def max_product_composition(A, B):
    """Compute max-product composition of matrices A and B."""
    rows_A, cols_A = A.shape
    rows_B, cols_B = B.shape
    assert cols_A == rows_B, "Number of columns in A must be equal to number of rows in B"

    C = np.zeros((rows_A, cols_B))
    for i in range(rows_A):
        for j in range(cols_B):
            C[i, j] = np.max(A[i, :] * B[:, j])

    return C

# Generate random matrices
matrix_A = generate_random_matrix(3, 3)
matrix_B = generate_random_matrix(3, 4)

print("Matrix A (3x3):")
print(matrix_A)

print("\nMatrix B (3x4):")
print(matrix_B)

# Compute compositions
composition_max_min = max_min_composition(matrix_A, matrix_B)
composition_max_product = max_product_composition(matrix_A, matrix_B)

print("\nMax-Min Composition Result:")
print(composition_max_min)

print("\nMax-Product Composition Result:")
print(composition_max_product)

#9  lab Experiment
"""Write a program that creates two random fuzzy sets of the dimensions say n and m (to be defined by the user)
Complete the fuzzy relation indexed by Cartesian product of the sets."""

import numpy as np

def generate_random_fuzzy_set(size):
    """Generate a fuzzy set with random values between 0 and 1."""
    return np.random.rand(size)

def create_fuzzy_relation(set1, set2):
    """Create a fuzzy relation indexed by the Cartesian product of set1 and set2."""
    n = len(set1)
    m = len(set2)
    relation = np.zeros((n, m))
    for i in range(n):
        for j in range(m):
            # The fuzzy relation can be defined in various ways.
            # Here, we will use the product of the fuzzy set values as a simple example.
            relation[i, j] = set1[i] * set2[j]
    return relation

def main():
    # User-defined dimensions for the fuzzy sets
    n = int(input("Enter the dimension for the first fuzzy set (n): "))
    m = int(input("Enter the dimension for the second fuzzy set (m): "))

    # Generate random fuzzy sets
    fuzzy_set1 = generate_random_fuzzy_set(n)
    fuzzy_set2 = generate_random_fuzzy_set(m)

    # Create fuzzy relation
    fuzzy_relation = create_fuzzy_relation(fuzzy_set1, fuzzy_set2)

    # Display results
    print("\nFuzzy Set 1:")
    print(fuzzy_set1)
    print("\nFuzzy Set 2:")
    print(fuzzy_set2)
    print("\nFuzzy Relation (indexed by Cartesian product):")
    print(fuzzy_relation)

if __name__ == "__main__":
    main()

#10  Genetic neuro hybrid systems, Genetic-Fuzzy rule based system.
import random

# Define the parameters of the genetic algorithm
population_size = 10
generations = 10
mutation_rate = 0.01

# Define the function to maximize
def fitness_function(x):
    return x**2

# Create an initial population
population = [random.uniform(-10, 10) for _ in range(population_size)]

def select_parents(population):
    # Select two parents using a tournament selection method
    parents = random.sample(population, 5)
    parents.sort(key=lambda x: fitness_function(x), reverse=True)
    return parents[:2]

def crossover(parent1, parent2):
    # Perform crossover by averaging the parents
    return (parent1 + parent2) / 2

def mutate(child):
    # Mutate the child with a small random value
    if random.random() < mutation_rate:
        return child + random.uniform(-1, 1)
    return child

# Run the genetic algorithm
for generation in range(generations):
    new_population = []
    for _ in range(population_size):
        parent1, parent2 = select_parents(population)
        child = crossover(parent1, parent2)
        child = mutate(child)
        new_population.append(child)
    population = new_population
    best_individual = max(population, key=fitness_function)
    print(f"Generation {generation}: Best Individual = {best_individual}, Fitness = {fitness_function(best_individual)}")

#11 lab Experiment
""" Consider a set P={p1, p2, p3, p4, p5} of five verities of plants. set D= {D1, D2, D3, D4, D5}
 of the various diseases affecting the plants
and S={s1, s2, s3, s4, s5} be the common symptoms. let R=P X D & Q = D X S"""
import numpy as np

# Define sets P, D, S
P = ['p1', 'p2', 'p3', 'p4', 'p5']
D = ['D1', 'D2', 'D3', 'D4', 'D5']
S = ['s1', 's2', 's3', 's4', 's5']

# Define relations R and Q
R = np.array([[1, 1, 1, 1, 1],  # p1 -> D1, D2, D3, D4, D5
              [1, 1, 1, 1, 1],  # p2 -> D1, D2, D3, D4, D5
              [1, 1, 1, 1, 1],  # p3 -> D1, D2, D3, D4, D5
              [1, 1, 1, 1, 1],  # p4 -> D1, D2, D3, D4, D5
              [1, 1, 1, 1, 1]]) # p5 -> D1, D2, D3, D4, D5

Q = np.array([[1, 1, 1, 1, 1],  # D1 -> s1, s2, s3, s4, s5
              [1, 1, 1, 1, 1],  # D2 -> s1, s2, s3, s4, s5
              [1, 1, 1, 1, 1],  # D3 -> s1, s2, s3, s4, s5
              [1, 1, 1, 1, 1],  # D4 -> s1, s2, s3, s4, s5
              [1, 1, 1, 1, 1]]) # D5 -> s1, s2, s3, s4, s5

# Composition of relations R and Q
composition_RQ = np.dot(R, Q)

# Display the results
print("Composition R ∘ Q:")
for i, p in enumerate(P):
    for j, s in enumerate(S):
        if composition_RQ[i, j] > 0:
            print(f"{p} is related to {s}")

#12 lab Experiment
"""Train an autocorrelator network for the pattern [1,-1,1,1] and also test the new weight for one missing
 and one mistake entry in the test vector respectively"""
import numpy as np

# Define the pattern to be detected
pattern = np.array([1, -1, 1, 1])

# Create a simple autocorrelator network
def autocorrelate(signal, pattern):
    correlation = np.correlate(signal, pattern, mode='valid')
    return correlation

# Test the network with a perfect match
perfect_match = np.array([1, -1, 1, 1])
correlation = autocorrelate(perfect_match, pattern)
print("Perfect match correlation:", correlation)

# Test with one missing entry
missing_entry = np.array([1, -1, 0, 1])
correlation = autocorrelate(missing_entry, pattern)
print("Missing entry correlation:", correlation)

# Test with one mistake entry
mistake_entry = np.array([1, -1, 1, -1])
correlation = autocorrelate(mistake_entry, pattern)
print("Mistake entry correlation:", correlation)

# 13 lab Experiment
"""Train the autocorrelator by given patterns: A1=(-1,1,-1,1), A2=(1,1,1,-1),
A3=(-1, -1, - 1, 1). Test it using patterns: Ax=(-1,1,-1,1),
Ay=(1,1,1,1), Az=(-1,-1,-1,-1)."""
import numpy as np

# Training patterns
A1 = np.array([-1, 1, -1, 1])
A2 = np.array([1, 1, 1, -1])
A3 = np.array([-1, -1, -1, 1])

# Correlation matrix
correlation_matrix = np.outer(A1, A1) + np.outer(A2, A2) + np.outer(A3, A3)

# Testing patterns
test_patterns = {
    "Ax": np.array([-1, 1, -1, 1]),
    "Ay": np.array([1, 1, 1, 1]),
    "Az": np.array([-1, -1, -1, -1])
}

# Testing the patterns
results = {}
for name, pattern in test_patterns.items():
    result = np.dot(correlation_matrix, pattern)
    results[name] = result

# Output results
for name, result in results.items():
    print(f"{name}: {result}")

#14 LAB Write a program in MATLAB to implement De-Morgan’s Law.
def de_morgan_laws(A, B):
    """
    Demonstrate De Morgan's Laws with Boolean values for sets A and B.
    Args:
        A (bool): Boolean value for set A.
        B (bool): Boolean value for set B.
    Returns:
        dict: Results of De Morgan's Laws for given A and B.
    """
    # Calculate the results according to De Morgan's Laws
    not_A = not A
    not_B = not B

    # De Morgan's Law 1: ¬(A ∪ B) = ¬A ∩ ¬B
    union_not = not (A or B)
    intersection_not = not_A and not_B

    # De Morgan's Law 2: ¬(A ∩ B) = ¬A ∪ ¬B
    intersection_not2 = not (A and B)
    union_not2 = not_A or not_B

    return {
        "¬(A ∪ B)": union_not,
        "¬A ∩ ¬B": intersection_not,
        "¬(A ∩ B)": intersection_not2,
        "¬A ∪ ¬B": union_not2
    }

def main():
    # Define boolean values for sets A and B
    A = bool(int(input("Enter 1 for True or 0 for False for A: ")))
    B = bool(int(input("Enter 1 for True or 0 for False for B: ")))

    # Calculate results
    results = de_morgan_laws(A, B)

    # Display results
    print("\nResults of De Morgan's Laws:")
    for law, result in results.items():
        print(f"{law}: {result}")

if __name__ == "__main__":
    main()

#15 Generate ANDNOT function using McCulloch-Pitts neural net.
def ANDNOT(A, B):
    # Weights
    w_A = 1
    w_B = -1

    # Threshold
    threshold = 0.5

    # Calculate the weighted sum
    weighted_sum = (w_A * A) + (w_B * B)

    # Apply the threshold
    output = 1 if weighted_sum >= threshold else 0

    return output

# Test the function with all input combinations
print("A B | A AND NOT B")
for A in [0, 1]:
    for B in [0, 1]:
        print(f"{A} {B} | {ANDNOT(A, B)}")

